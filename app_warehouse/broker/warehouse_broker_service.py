# -*- coding: utf-8 -*-
"""Servicios de integraci√≥n de warehouse con RabbitMQ.

Este m√≥dulo define las corrutinas que se conectan a RabbitMQ, declaran
colas y consumen eventos relacionados con el almac√©n.
"""

import asyncio
import json
import logging

from aio_pika import Message  # Podr√°s usarlo m√°s adelante para publicar eventos.
from microservice_chassis_grupo2.core.rabbitmq_core import (
    get_channel,
    declare_exchange,
    declare_exchange_logs,
)

logger = logging.getLogger(__name__)


async def consume_process_canceled_events():
    """Consume eventos de procesos cancelados desde RabbitMQ.

    Esta corrutina:
    - Abre un canal contra RabbitMQ (via `get_channel`).
    - Declara el exchange principal (topic).
    - Declara y enlaza la cola `process_canceled_queue` con la routing key
      `process.canceled`.
    - Asocia la cola con la funci√≥n `handle_process_canceled`.
    - Se queda bloqueada para escuchar eventos indefinidamente.
    """
    try:
        logger.info("[WAREHOUSE] üîÑ Iniciando consume_process_canceled_events...")
        print("[WAREHOUSE] üîÑ Iniciando consume_process_canceled_events...", flush=True)

        # Obtenemos conexi√≥n y canal al broker
        _, channel = await get_channel()

        # Exchange principal (mismo patr√≥n que delivery)
        exchange = await declare_exchange(channel)

        # Cola donde esperamos que otro servicio publique `process.canceled`
        queue = await channel.declare_queue("process_canceled_queue", durable=True)
        await queue.bind(exchange, routing_key="process.canceled")

        # Registramos el callback
        await queue.consume(handle_process_canceled)

        logger.info("[WAREHOUSE] üü¢ Escuchando eventos process.canceled...")
        print("[WAREHOUSE] üü¢ Escuchando eventos process.canceled...", flush=True)

        # Mantener la corrutina viva
        await asyncio.Future()

    except Exception as exc:  # noqa: BLE001
        logger.error(
            "[WAREHOUSE] ‚ùå Error en consume_process_canceled_events: %s",
            exc,
            exc_info=True,
        )
        print(f"[WAREHOUSE] ‚ùå Error en consume_process_canceled_events: {exc}", flush=True)


async def handle_process_canceled(message):
    """Procesa un evento de proceso cancelado.

    La estructura exacta del mensaje depender√° de c√≥mo lo publique el
    microservicio de procesos/m√°quinas. Para el primer commit, nos
    limitamos a:
    - Parsear el body como JSON.
    - Loguear el contenido.
    - Dejar un TODO para, en siguientes pasos, llamar a la capa de
      negocio/CRUD de warehouse.
    """
    async with message.process():
        try:
            # Decodificar el mensaje recibido
            data = json.loads(message.body)

            # Ejemplo de estructura esperada (ajustable m√°s adelante):
            # {
            #   "process_id": 123,
            #   "piece_type": "A",
            #   "quantity": 10
            # }
            process_id = data.get("process_id")
            piece_type = data.get("piece_type")
            quantity = data.get("quantity")

            logger.info(
                "[WAREHOUSE] üì• Proceso cancelado recibido: process_id=%s "
                "piece_type=%s quantity=%s",
                process_id,
                piece_type,
                quantity,
            )

            # üîß Aqu√≠, en iteraciones futuras:
            # - Llamar a un servicio/CRUD para registrar las piezas en almac√©n.
            #   Ejemplo:
            #   await warehouse_service.store_canceled_pieces(
            #       process_id=process_id,
            #       piece_type=piece_type,
            #       quantity=quantity,
            #   )

        except Exception as exc:  # noqa: BLE001
            logger.error(
                "[WAREHOUSE] ‚ùå Error procesando evento process.canceled: %s",
                exc,
                exc_info=True,
            )
            await publish_to_logger(
                message={"message": "Error procesando process.canceled", "error": str(exc)},
                topic="warehouse.error",
            )


async def publish_to_logger(message: dict, topic: str):
    """Publica mensajes de log en el exchange de logs.

    Esto permite integrar los logs de warehouse en el sistema de logging
    centralizado del proyecto, igual que hace delivery.
    """
    connection = None
    try:
        # Abrimos conexi√≥n y canal contra el broker
        connection, channel = await get_channel()

        # Declaramos/obtenemos el exchange de logs
        exchange = await declare_exchange_logs(channel)

        # Serializamos el mensaje a JSON
        body = json.dumps(message).encode()

        # Construimos el mensaje RabbitMQ persistente
        msg = Message(
            body=body,
            content_type="application/json",
            delivery_mode=2,  # persistente
        )

        # Publicamos usando el topic proporcionado
        await exchange.publish(message=msg, routing_key=topic)

    except Exception as exc:  # noqa: BLE001
        logger.error("[WAREHOUSE] ‚ùå Error publicando en logger: %s", exc, exc_info=True)
    finally:
        if connection:
            await connection.close()
